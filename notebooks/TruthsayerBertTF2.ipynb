{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TruthsayerBertTF2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhFouPty3vv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow as tf\n",
        "\n",
        "#print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEwnpA8sxoWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior()\n",
        "\n",
        "#%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4COgT0bVHBM",
        "colab_type": "code",
        "outputId": "69a8ddb7-ffb8-4110-9cde-c1f851196f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow_hub\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.28.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.4MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.2.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (46.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f8a8863b6e5b34e4fb5081b9fb2aa73b03a4f40d1b7b91a4c22be7a0b13851dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (46.1.3)\n",
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 2.8MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.38.0)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=7f415df88f63f3f1ba228edcfb347a8dab14d923829a267daee618735077a7c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=fadf6a97fb8b040be6a4a587bd0d50ce85a4beb772d0d9bc40f9c27e89b9f317\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=7fe1ff2fc0bd14586ceedca34e32d80a1bde2928cbe2519ca568ccbcf209db4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "outputId": "f4074b3d-5913-4af1-fdea-aac287149110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "import bert\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math\n",
        "\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version:  2.0.0\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_N3zC1UVtkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tf-hub-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "5057cdfe-b343-4b57-c62e-4d9533398004",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'bert-sentiment'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = True #@param {type:\"boolean\"}\n",
        "BUCKET = 'bert_truthsayer_test1' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.io.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_truthsayer_test1/bert-sentiment *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w45hJz0W4HS",
        "colab_type": "code",
        "outputId": "108df2b0-929f-4be3-967a-f195e42531a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL2NF2SBZu3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n",
        "!rm -rf data\n",
        "!rm -rf data_zombie\n",
        "!rm -rf data_combined\n",
        "!rm -rf neg\n",
        "!rm -rf pos\n",
        "!tar -zxvf /content/gdrive/My\\ Drive/NLP_Projects/truthsayer/data.tar.gz\n",
        "!tar -zxvf /content/gdrive/My\\ Drive/NLP_Projects/truthsayer/data_combined.tar.gz\n",
        "!tar -zxvf /content/gdrive/My\\ Drive/NLP_Projects/truthsayer/data_zombie.tar.gz\n",
        "#!mkdir data_combined\n",
        "#!mv neg data_combined/\n",
        "#!mv pos data_combined/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPf6tC8z0UgR",
        "colab_type": "code",
        "outputId": "d1561d39-ed45-42b9-ce5a-9c2141fd7f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls data_combined/*/ | wc -l\n",
        "!ls data/*/ | wc -l\n",
        "!ls data_zombie/*/ | wc -l\n",
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27266\n",
            "16691\n",
            "10575\n",
            "adc.json  data\tdata_combined  data_zombie  gdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  numFails = 0\n",
        "  numSuccess = 0\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      try:\n",
        "        sentence = f.read()\n",
        "        sentiment = int(re.match(\".*_(\\d+)\", file_path).group(1))\n",
        "      except:\n",
        "        numFails = numFails + 1\n",
        "        print(\"failed to decode sentence\")\n",
        "      \n",
        "      data[\"sentence\"].append(sentence)\n",
        "      #data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "      data[\"sentiment\"].append(sentiment)\n",
        "      numSuccess = numSuccess + 1\n",
        "\n",
        "  print(directory+\" success/fails: \"+str(numSuccess)+\"/\"+str(numFails))\n",
        "  print(\"data[sentence]: \" + str(len(data[\"sentence\"])))\n",
        "  print(\"data[sentiment]: \" + str(len(data[\"sentiment\"])))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(datadir, force_download=False):\n",
        "  df = load_dataset(os.path.join(datadir))\n",
        "  print(df.head())\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXKKdMYdxri7",
        "colab_type": "code",
        "outputId": "ae40be94-5705-47bc-9622-11f3b1b322c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "allData = download_and_load_datasets(\"data\")\n",
        "shuffledData = allData.sample(10000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pos success/fails: 12273/0\n",
            "data[sentence]: 12273\n",
            "data[sentiment]: 12273\n",
            "data/neg success/fails: 4415/0\n",
            "data[sentence]: 4415\n",
            "data[sentiment]: 4415\n",
            "                                            sentence  sentiment  polarity\n",
            "0  To conservatives like Esenberg, prioritizing t...         10         1\n",
            "1  A survey by Seaton Hall University finds that ...          1         0\n",
            "2  Donald Trump lost an estimated $1bn of his pap...          9         1\n",
            "3  'It's Like Walking Into Chernobyl,' One Doctor...         10         1\n",
            "4  This was my first nursing job. How crazy is th...          7         1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "source": [
        "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train = train.sample(10000)\n",
        "#dev = dev.sample(150)\n",
        "#test = test.sample(150)\n",
        "train = shuffledData[0:9000]\n",
        "dev = shuffledData[9001:9500]\n",
        "test = shuffledData[9501:9999]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab_type": "code",
        "outputId": "b1e00f7a-219c-439c-eb85-51a1dd8610c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.columns\n",
        "train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2633</th>\n",
              "      <td>Image copyright Alex Hilton Image caption IT c...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14926</th>\n",
              "      <td>Electricity use drops, but not at homeWhat hap...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11398</th>\n",
              "      <td>Millennial Love is The Independent lifestyle d...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15370</th>\n",
              "      <td>Soaring death toll has been fueled by adding 3...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4337</th>\n",
              "      <td>GRE Statistics: the four most important concep...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  sentiment  polarity\n",
              "2633   Image copyright Alex Hilton Image caption IT c...         10         1\n",
              "14926  Electricity use drops, but not at homeWhat hap...          6         1\n",
              "11398  Millennial Love is The Independent lifestyle d...          6         1\n",
              "15370  Soaring death toll has been fueled by adding 3...          9         1\n",
              "4337   GRE Statistics: the four most important concep...         10         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz",
        "colab_type": "text"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'sentiment'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdt8svheYn2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                         text_a = x[DATA_COLUMN], \n",
        "                                                         text_b = None, \n",
        "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "dev_InputExamples = dev.apply(lambda x: InputExample(guid=None, \n",
        "                                                     text_a = x[DATA_COLUMN], \n",
        "                                                     text_b = None, \n",
        "                                                     label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: InputExample(guid=None, \n",
        "                                                       text_a = x[DATA_COLUMN], \n",
        "                                                       text_b = None, \n",
        "                                                       label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2saD-qZd5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 128  # Your choice here.\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp0tjM6OZX_y",
        "colab_type": "code",
        "outputId": "8224fd5f-d2d2-4b6e-9155-ca1c68f93cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ln7AtjmbV2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features\n",
        "\n",
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    print(\"*** Example ***\")\n",
        "    print(\"guid: %s\" % (example.guid))\n",
        "    print(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "    print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    print(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      is_real_example=True)\n",
        "  return feature\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "                input_ids,\n",
        "                input_mask,\n",
        "                segment_ids,\n",
        "                label_id,\n",
        "                is_real_example=True):\n",
        "    self.input_ids = input_ids  \n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "11ba3132-d4a2-4d48-c213-e27c781f3582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = convert_examples_to_features(train_InputExamples, label_list, max_seq_length, tokenizer)\n",
        "dev_features = convert_examples_to_features(dev_InputExamples, label_list, max_seq_length, tokenizer)\n",
        "test_features = convert_examples_to_features(test_InputExamples, label_list, max_seq_length, tokenizer)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing example 0 of 9000\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] image copyright alex hilton image capt ##ion it consultant alex hilton with wife jenny and sons joe and james ##bri ##tish consumers say they are struggling to get ref ##unds on cancelled holidays due to the corona ##virus outbreak . it consultant alex hilton and his wife jenny had planned a skiing holiday in france with university friends they had stayed in touch with over many years . last july , they made a booking for 20 people to stay at the cha ##let amelia in val d ' is ##ere this april . the total cost of the cha ##let alone came to £1 ##7 , 000 . the corona ##virus lock ##down means they cannot go . but they have been told that they [SEP]\n",
            "input_ids: 101 3746 9385 4074 15481 3746 14408 3258 2009 8930 4074 15481 2007 2564 8437 1998 4124 3533 1998 2508 23736 24788 10390 2360 2027 2024 8084 2000 2131 25416 26698 2006 8014 11938 2349 2000 1996 21887 23350 8293 1012 2009 8930 4074 15481 1998 2010 2564 8437 2018 3740 1037 12701 6209 1999 2605 2007 2118 2814 2027 2018 4370 1999 3543 2007 2058 2116 2086 1012 2197 2251 1010 2027 2081 1037 21725 2005 2322 2111 2000 2994 2012 1996 15775 7485 11556 1999 11748 1040 1005 2003 7869 2023 2258 1012 1996 2561 3465 1997 1996 15775 7485 2894 2234 2000 14534 2581 1010 2199 1012 1996 21887 23350 5843 7698 2965 2027 3685 2175 1012 2021 2027 2031 2042 2409 2008 2027 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] electricity use drops , but not at home ##w ##hat happens when most people quit going to work ? not surprisingly , power use drops , overall . hawaiian electric reported thursday that the average peak demand for the week of march 22 , about when the stay - at - home order was issued , was 7 % less than previous weeks for o ##ahu and hawaii island ; it was down 14 % for maui . the individual home use , though , might be a surprise , with more lights on , stove ##s heating up and electronic devices running than usual . deep breath , when you open that bill . an easter sunday church drive - in ##fi ##rst assembly of [SEP]\n",
            "input_ids: 101 6451 2224 9010 1010 2021 2025 2012 2188 2860 12707 6433 2043 2087 2111 8046 2183 2000 2147 1029 2025 10889 1010 2373 2224 9010 1010 3452 1012 12188 3751 2988 9432 2008 1996 2779 4672 5157 2005 1996 2733 1997 2233 2570 1010 2055 2043 1996 2994 1011 2012 1011 2188 2344 2001 3843 1010 2001 1021 1003 2625 2084 3025 3134 2005 1051 21463 1998 7359 2479 1025 2009 2001 2091 2403 1003 2005 28566 1012 1996 3265 2188 2224 1010 2295 1010 2453 2022 1037 4474 1010 2007 2062 4597 2006 1010 16247 2015 10808 2039 1998 4816 5733 2770 2084 5156 1012 2784 3052 1010 2043 2017 2330 2008 3021 1012 2019 10957 4465 2277 3298 1011 1999 8873 12096 3320 1997 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 6 (id = 6)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] millennia ##l love is the independent lifestyle desk ’ s fort ##night ##ly dating and relationships podcast . the podcast features in - depth discussions surrounding sexuality , identity and gender , with topics discussed including how to have feminist sex , keeping the spark alive in long - term relationships and dating in a post - # met ##oo era . in the latest episode , listeners of the show open up about how corona ##virus has impacted their relationships . download the new independent premium app sharing the full story , not just the headlines ##we hear from a woman who has been forced to continue living with her ex - boyfriend due to the uk lock ##down and a french student based in [SEP]\n",
            "input_ids: 101 27620 2140 2293 2003 1996 2981 9580 4624 1521 1055 3481 15864 2135 5306 1998 6550 16110 1012 1996 16110 2838 1999 1011 5995 10287 4193 13798 1010 4767 1998 5907 1010 2007 7832 6936 2164 2129 2000 2031 10469 3348 1010 4363 1996 12125 4142 1999 2146 1011 2744 6550 1998 5306 1999 1037 2695 1011 1001 2777 9541 3690 1012 1999 1996 6745 2792 1010 13810 1997 1996 2265 2330 2039 2055 2129 21887 23350 2038 19209 2037 6550 1012 8816 1996 2047 2981 12882 10439 6631 1996 2440 2466 1010 2025 2074 1996 19377 8545 2963 2013 1037 2450 2040 2038 2042 3140 2000 3613 2542 2007 2014 4654 1011 6898 2349 2000 1996 2866 5843 7698 1998 1037 2413 3076 2241 1999 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 6 (id = 6)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] soaring death toll has been fueled by adding 3 , 77 ##8 people who weren ’ t tested for co ##vid - 19 but are presumed to have died from it ##ne ##w york city has revised its co ##vid - 19 death toll sharply upwards to more than 10 , 000 people , with the city now firmly established as being at the heart of the global corona ##virus crisis . the soaring death toll has been fueled by the adding of 3 , 77 ##8 people who weren ’ t tested for co ##vid - 19 but are presumed to have died from it . last week , bill de b ##las ##io , new york city ’ s mayor , admitted that the [SEP]\n",
            "input_ids: 101 23990 2331 9565 2038 2042 17999 2011 5815 1017 1010 6255 2620 2111 2040 4694 1521 1056 7718 2005 2522 17258 1011 2539 2021 2024 14609 2000 2031 2351 2013 2009 2638 2860 2259 2103 2038 8001 2049 2522 17258 1011 2539 2331 9565 9249 14873 2000 2062 2084 2184 1010 2199 2111 1010 2007 1996 2103 2085 7933 2511 2004 2108 2012 1996 2540 1997 1996 3795 21887 23350 5325 1012 1996 23990 2331 9565 2038 2042 17999 2011 1996 5815 1997 1017 1010 6255 2620 2111 2040 4694 1521 1056 7718 2005 2522 17258 1011 2539 2021 2024 14609 2000 2031 2351 2013 2009 1012 2197 2733 1010 3021 2139 1038 8523 3695 1010 2047 2259 2103 1521 1055 3664 1010 4914 2008 1996 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 9 (id = 9)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] gr ##e statistics : the four most important concepts to know ##the thought of dealing with statistics questions on the gr ##e can make even the sa ##v ##vies ##t of test - take ##rs cr ##inge . worry not : in this blog post , we ’ ll walk you through four integral gr ##e statistics concepts — mean , median , mode , standard deviation — so you can tackle them with confidence on exam day . 1 . mean ##a ‘ mean ’ is the average of a set of numbers . let ’ s take a look at the set below , which outlines the number of cookies a group of people can eat over the course of a night . bob [SEP]\n",
            "input_ids: 101 24665 2063 6747 1024 1996 2176 2087 2590 8474 2000 2113 10760 2245 1997 7149 2007 6747 3980 2006 1996 24665 2063 2064 2191 2130 1996 7842 2615 25929 2102 1997 3231 1011 2202 2869 13675 23496 1012 4737 2025 1024 1999 2023 9927 2695 1010 2057 1521 2222 3328 2017 2083 2176 9897 24665 2063 6747 8474 1517 2812 1010 3991 1010 5549 1010 3115 24353 1517 2061 2017 2064 11147 2068 2007 7023 2006 11360 2154 1012 1015 1012 2812 2050 1520 2812 1521 2003 1996 2779 1997 1037 2275 1997 3616 1012 2292 1521 1055 2202 1037 2298 2012 1996 2275 2917 1010 2029 22106 1996 2193 1997 16324 1037 2177 1997 2111 2064 4521 2058 1996 2607 1997 1037 2305 1012 3960 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "Writing example 0 of 499\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] on wednesday , trump accused the agency of will ##fully doing china ’ s bidding , at the expense of global health , as he justified the hold ##up in funding . ada ##d “ took them a long time to realize what was going , but i have a feeling they knew exactly what was going , ” trump said at the daily white house corona ##virus briefing , where he again to ##uted the ban he imposed on most air travel from china in late january over who objections . trump ’ s allies argued that the move against the who is consistent with the administration ’ s immediate goals of counter ##ing chinese propaganda over the origins and spread of the virus and [SEP]\n",
            "input_ids: 101 2006 9317 1010 8398 5496 1996 4034 1997 2097 7699 2725 2859 1521 1055 17534 1010 2012 1996 10961 1997 3795 2740 1010 2004 2002 15123 1996 2907 6279 1999 4804 1012 15262 2094 1523 2165 2068 1037 2146 2051 2000 5382 2054 2001 2183 1010 2021 1045 2031 1037 3110 2027 2354 3599 2054 2001 2183 1010 1524 8398 2056 2012 1996 3679 2317 2160 21887 23350 27918 1010 2073 2002 2153 2000 12926 1996 7221 2002 9770 2006 2087 2250 3604 2013 2859 1999 2397 2254 2058 2040 17304 1012 8398 1521 1055 6956 5275 2008 1996 2693 2114 1996 2040 2003 8335 2007 1996 3447 1521 1055 6234 3289 1997 4675 2075 2822 10398 2058 1996 7321 1998 3659 1997 1996 7865 1998 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 7 (id = 7)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] image copyright get ##ty images image capt ##ion mast ##s have been targeted in birmingham and mer ##sey ##side ##mobile phone mast ##s have been torch ##ed amid theories linking corona ##virus to 5 ##g , despite ministers saying there is no credible evidence to back them . mast ##s were set ali ##ght in spark ##hill , birmingham , on thursday and mel ##ling , mer ##sey ##side , on friday . trade body mobile uk said false rumours and theories linking 5 ##g and corona ##virus were \" concerning \" . the government said \" there is absolutely no credible evidence of a link \" between the two . posting on twitter , the department for digital , culture , media and sport added [SEP]\n",
            "input_ids: 101 3746 9385 2131 3723 4871 3746 14408 3258 15429 2015 2031 2042 9416 1999 6484 1998 21442 7952 7363 17751 3042 15429 2015 2031 2042 12723 2098 13463 8106 11383 21887 23350 2000 1019 2290 1010 2750 7767 3038 2045 2003 2053 23411 3350 2000 2067 2068 1012 15429 2015 2020 2275 4862 13900 1999 12125 7100 1010 6484 1010 2006 9432 1998 11463 2989 1010 21442 7952 7363 1010 2006 5958 1012 3119 2303 4684 2866 2056 6270 19200 1998 8106 11383 1019 2290 1998 21887 23350 2020 1000 7175 1000 1012 1996 2231 2056 1000 2045 2003 7078 2053 23411 3350 1997 1037 4957 1000 2090 1996 2048 1012 14739 2006 10474 1010 1996 2533 2005 3617 1010 3226 1010 2865 1998 4368 2794 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] geneva ( af ##p ) – as co ##vid - 19 captures global headlines , the world health organization ( who ) warned tuesday that the world needs nearly six million nurses . the un ’ s health agency along with partners nursing now and the international council of nurses ( ic ##n ) under ##sco ##red in a report the crucial role played by nurses , who make up more than half of all health workers worldwide . “ nurses are the backbone of any health system , ” who chief ted ##ros ad ##han ##om g ##he ##bre ##yes ##us said in a statement . “ today , many nurses find themselves on the front ##line in the battle against co ##vid - 19 [SEP]\n",
            "input_ids: 101 9810 1006 21358 2361 1007 1516 2004 2522 17258 1011 2539 19566 3795 19377 1010 1996 2088 2740 3029 1006 2040 1007 7420 9857 2008 1996 2088 3791 3053 2416 2454 11500 1012 1996 4895 1521 1055 2740 4034 2247 2007 5826 8329 2085 1998 1996 2248 2473 1997 11500 1006 24582 2078 1007 2104 9363 5596 1999 1037 3189 1996 10232 2535 2209 2011 11500 1010 2040 2191 2039 2062 2084 2431 1997 2035 2740 3667 4969 1012 1523 11500 2024 1996 21505 1997 2151 2740 2291 1010 1524 2040 2708 6945 7352 4748 4819 5358 1043 5369 13578 23147 2271 2056 1999 1037 4861 1012 1523 2651 1010 2116 11500 2424 3209 2006 1996 2392 4179 1999 1996 2645 2114 2522 17258 1011 2539 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 1 (id = 1)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] a political divide over the response to the corona ##virus pan ##de ##mic was on stark display wednesday in lansing , michigan , where about 3 , 000 conservative demonstrators rallied against the democratic governor ’ s policies . “ operation grid ##lock ” wasn ’ t intended to go beyond people driving around and hon ##king their horns in opposition to gov . gretchen w ##hit ##mer ’ s executive stay - at - home order , organizers say , but some drivers got out of their cars and the event took on a flavor of a trump rally , with some demonstrators , including members of militia groups , protesting close together while chanting slogan ##s like “ lock her up , ” referencing [SEP]\n",
            "input_ids: 101 1037 2576 11443 2058 1996 3433 2000 1996 21887 23350 6090 3207 7712 2001 2006 9762 4653 9317 1999 22304 1010 4174 1010 2073 2055 1017 1010 2199 4603 28337 24356 2114 1996 3537 3099 1521 1055 6043 1012 1523 3169 8370 7878 1524 2347 1521 1056 3832 2000 2175 3458 2111 4439 2105 1998 10189 6834 2037 11569 1999 4559 2000 18079 1012 21625 1059 16584 5017 1521 1055 3237 2994 1011 2012 1011 2188 2344 1010 18829 2360 1010 2021 2070 6853 2288 2041 1997 2037 3765 1998 1996 2724 2165 2006 1037 14894 1997 1037 8398 8320 1010 2007 2070 28337 1010 2164 2372 1997 8396 2967 1010 21248 2485 2362 2096 22417 14558 2015 2066 1523 5843 2014 2039 1010 1524 28789 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 7 (id = 7)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] file photo : norway ' s prime minister er ##na sol ##berg attends a panel during the annual munich security conference in munich , germany february 14 , 2020 . reuters / andreas ge ##bert / file photo ##os ##lo ( reuters ) - norway will lift some of the restrictions imposed to prevent the spread of the novel corona ##virus , the country ’ s prime minister said on tuesday . “ together we have taken control of the virus , therefore we can open up society little by little , ” er ##na sol ##berg told a news conference . on monday , her health minister said the epidemic was “ under control ” in norway , pointing to the low rate of transmission [SEP]\n",
            "input_ids: 101 5371 6302 1024 5120 1005 1055 3539 2704 9413 2532 14017 4059 23933 1037 5997 2076 1996 3296 7469 3036 3034 1999 7469 1010 2762 2337 2403 1010 12609 1012 26665 1013 12460 16216 8296 1013 5371 6302 2891 4135 1006 26665 1007 1011 5120 2097 6336 2070 1997 1996 9259 9770 2000 4652 1996 3659 1997 1996 3117 21887 23350 1010 1996 2406 1521 1055 3539 2704 2056 2006 9857 1012 1523 2362 2057 2031 2579 2491 1997 1996 7865 1010 3568 2057 2064 2330 2039 2554 2210 2011 2210 1010 1524 9413 2532 14017 4059 2409 1037 2739 3034 1012 2006 6928 1010 2014 2740 2704 2056 1996 16311 2001 1523 2104 2491 1524 1999 5120 1010 7302 2000 1996 2659 3446 1997 6726 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "Writing example 0 of 498\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] it ’ s easy to see debt and financial worries as a generation ##al issue . millennia ##ls are often singled out as the un ##lu ##cky generation – hit by a multitude of factors including university fees , high rents and high house prices . but here ’ s a growing core of people of every age group who are facing real financial hardship . people struggling is not a new phenomenon , of course . there have been poor people of every age as long as society has existed . it is und ##enia ##ble that people under 40 , those born between 1981 and 1996 , are finding finances hard . download the new independent premium app sharing the full story , not [SEP]\n",
            "input_ids: 101 2009 1521 1055 3733 2000 2156 7016 1998 3361 15508 2004 1037 4245 2389 3277 1012 27620 4877 2024 2411 25369 2041 2004 1996 4895 7630 17413 4245 1516 2718 2011 1037 20889 1997 5876 2164 2118 9883 1010 2152 28206 1998 2152 2160 7597 1012 2021 2182 1521 1055 1037 3652 4563 1997 2111 1997 2296 2287 2177 2040 2024 5307 2613 3361 26479 1012 2111 8084 2003 2025 1037 2047 9575 1010 1997 2607 1012 2045 2031 2042 3532 2111 1997 2296 2287 2004 2146 2004 2554 2038 5839 1012 2009 2003 6151 19825 3468 2008 2111 2104 2871 1010 2216 2141 2090 3261 1998 2727 1010 2024 4531 16156 2524 1012 8816 1996 2047 2981 12882 10439 6631 1996 2440 2466 1010 2025 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 6 (id = 6)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] thousands of wisconsin poll workers said they will not work next tuesday as the state refuses to delay voting and moves forward with election day during the deadly corona ##virus pan ##de ##mic . “ i ’ m scared , ” said 43 - year - old angie copa ##s , who suffers from high blood pressure and asthma . “ on election day , we ’ re exposed to them and everyone they come across . i ’ m the only person in the office and i run it all . ” the new york times noted : as of tuesday , roughly 7 , 000 had said they would not work , and 111 precinct ##s did not have a single poll worker on [SEP]\n",
            "input_ids: 101 5190 1997 5273 8554 3667 2056 2027 2097 2025 2147 2279 9857 2004 1996 2110 10220 2000 8536 6830 1998 5829 2830 2007 2602 2154 2076 1996 9252 21887 23350 6090 3207 7712 1012 1523 1045 1521 1049 6015 1010 1524 2056 4724 1011 2095 1011 2214 14835 10613 2015 1010 2040 17567 2013 2152 2668 3778 1998 26180 1012 1523 2006 2602 2154 1010 2057 1521 2128 6086 2000 2068 1998 3071 2027 2272 2408 1012 1045 1521 1049 1996 2069 2711 1999 1996 2436 1998 1045 2448 2009 2035 1012 1524 1996 2047 2259 2335 3264 1024 2004 1997 9857 1010 5560 1021 1010 2199 2018 2056 2027 2052 2025 2147 1010 1998 11118 18761 2015 2106 2025 2031 1037 2309 8554 7309 2006 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 1 (id = 1)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] file photo : irish finance minister pas ##chal don ##oh ##oe walks outside government buildings in dublin , ireland october 8 , 2019 . reuters / lorraine o ' sullivan ##du ##bl ##in ( reuters ) - ireland will amend its temporary wage subsidy scheme in order to pay a higher percentage of low income workers and to encourage a greater take - up by employers , finance minister pas ##chal don ##oh ##oe said on wednesday . around 250 , 000 employees have been retained on the payroll ##s of around 43 , 000 companies during the economic shut ##down caused by the corona ##virus outbreak , with the irish state paying 70 % of wages up to a maximum of 410 euros ( $ [SEP]\n",
            "input_ids: 101 5371 6302 1024 3493 5446 2704 14674 18598 2123 11631 8913 7365 2648 2231 3121 1999 5772 1010 3163 2255 1022 1010 10476 1012 26665 1013 13895 1051 1005 7624 8566 16558 2378 1006 26665 1007 1011 3163 2097 27950 2049 5741 11897 28768 5679 1999 2344 2000 3477 1037 3020 7017 1997 2659 3318 3667 1998 2000 8627 1037 3618 2202 1011 2039 2011 12433 1010 5446 2704 14674 18598 2123 11631 8913 2056 2006 9317 1012 2105 5539 1010 2199 5126 2031 2042 6025 2006 1996 26854 2015 1997 2105 4724 1010 2199 3316 2076 1996 3171 3844 7698 3303 2011 1996 21887 23350 8293 1010 2007 1996 3493 2110 7079 3963 1003 1997 12678 2039 2000 1037 4555 1997 19151 19329 1006 1002 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] o ##cci ##dent ##al petroleum corp . has hired a new finance chief , part of a wider management shake - up that comes amid renewed criticism of the company ’ s $ 38 billion deal to buy rival ana ##dar ##ko petroleum corp . the houston - based company said friday it has promoted robert peterson as its next chief financial officer , succeeding cedric bu ##rgh ##er . mr . peterson has served as senior vice president of the per ##mian enhanced oil recovery techniques unit for o ##cci ##dent ##al oil and gas since september . previously , he was the vice president of per ##mian strategy for o ##cci ##dent ##al oil and gas . newsletter sign - up cf ##o journal [SEP]\n",
            "input_ids: 101 1051 14693 16454 2389 11540 13058 1012 2038 5086 1037 2047 5446 2708 1010 2112 1997 1037 7289 2968 6073 1011 2039 2008 3310 13463 9100 6256 1997 1996 2194 1521 1055 1002 4229 4551 3066 2000 4965 6538 9617 7662 3683 11540 13058 1012 1996 5395 1011 2241 2194 2056 5958 2009 2038 3755 2728 12001 2004 2049 2279 2708 3361 2961 1010 13034 26170 20934 27172 2121 1012 2720 1012 12001 2038 2366 2004 3026 3580 2343 1997 1996 2566 20924 9412 3514 7233 5461 3131 2005 1051 14693 16454 2389 3514 1998 3806 2144 2244 1012 3130 1010 2002 2001 1996 3580 2343 1997 2566 20924 5656 2005 1051 14693 16454 2389 3514 1998 3806 1012 17178 3696 1011 2039 12935 2080 3485 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 10 (id = 10)\n",
            "*** Example ***\n",
            "guid: None\n",
            "tokens: [CLS] the fundamental obligation of any government is to keep its citizens safe . in a complex , representative democracy like the united states , these tasks must be delegate ##d to public officials who have the expertise and judgment to do so honestly , transparent ##ly and competent ##ly , while the rest of us can go about the business of our daily lives . as michael lewis brilliant ##ly illustrated in the fifth risk , the us career bureaucracy is filled with people who are experts in anticipating disasters and planning how to avoid or mit ##igate them . yet political leadership has repeatedly and at the worst possible times failed in performing its essential function of ass ##uring that the american people are safe [SEP]\n",
            "input_ids: 101 1996 8050 14987 1997 2151 2231 2003 2000 2562 2049 4480 3647 1012 1999 1037 3375 1010 4387 7072 2066 1996 2142 2163 1010 2122 8518 2442 2022 11849 2094 2000 2270 4584 2040 2031 1996 11532 1998 8689 2000 2079 2061 9826 1010 13338 2135 1998 17824 2135 1010 2096 1996 2717 1997 2149 2064 2175 2055 1996 2449 1997 2256 3679 3268 1012 2004 2745 4572 8235 2135 7203 1999 1996 3587 3891 1010 1996 2149 2476 25934 2003 3561 2007 2111 2040 2024 8519 1999 26481 18665 1998 4041 2129 2000 4468 2030 10210 28731 2068 1012 2664 2576 4105 2038 8385 1998 2012 1996 5409 2825 2335 3478 1999 4488 2049 6827 3853 1997 4632 12228 2008 1996 2137 2111 2024 3647 102\n",
            "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label: 6 (id = 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  print(\"output_layer.shape[-1]:\"+str(output_layer.shape[-1]))\n",
        "  hidden_size = output_layer.shape[-1]\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.compat.v1.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.compat.v1.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.compat.v1.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, rate=.1)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75DOSR9gKT7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.compat.v1.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  tvars = tf.compat.v1.trainable_variables()\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.compat.v1.get_variable(\n",
        "          name=param_name + \"/adam_m\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.compat.v1.get_variable(\n",
        "          name=param_name + \"/adam_v\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.compat.v1.metrics.accuracy(label_ids, predicted_labels)\n",
        "        #f1_score = tf.contrib.metrics.f1_score(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        #auc = tf.metrics.auc(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        #recall = tf.metrics.recall(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        #precision = tf.metrics.precision(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels) \n",
        "        #true_pos = tf.metrics.true_positives(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        #true_neg = tf.metrics.true_negatives(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)   \n",
        "        #false_pos = tf.metrics.false_positives(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)  \n",
        "        #false_neg = tf.metrics.false_negatives(\n",
        "        #    label_ids,\n",
        "        #    predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "        #    \"f1_score\": f1_score,\n",
        "          #\"auc\": auc,\n",
        "        #    \"precision\": precision,\n",
        "        #    \"recall\": recall,\n",
        "        #    \"true_positives\": true_pos,\n",
        "        #    \"true_negatives\": true_neg,\n",
        "        #    \"false_positives\": false_pos,\n",
        "        #    \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "#NUM_TRAIN_EPOCHS = 3.0\n",
        "NUM_TRAIN_EPOCHS = 6.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "outputId": "4c83b94d-8c58-4b83-ccc6-dda33ac92121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "print(\"len(train_features): \"+str(len(train_features)))\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "print(\"num_train_steps: \"+str(num_train_steps))\n",
        "print(\"num_warmup_steps: \" +str(num_warmup_steps))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_features): 9000\n",
            "num_train_steps: 1687\n",
            "num_warmup_steps: 168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "ef71ee54-ef67-47b5-bc47-b12d762e980f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bert_truthsayer_test1/bert-sentiment', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa153a632e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bert_truthsayer_test1/bert-sentiment', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa153a632e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo",
        "colab_type": "text"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggf-fZyCeSsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is not used by this file but is still used by the Colab and\n",
        "# people who depend on it.\n",
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "4584d46c-d43d-4327-b868-dc60ec1a13d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "output_layer.shape[-1]:768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into gs://bert_truthsayer_test1/bert-sentiment/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into gs://bert_truthsayer_test1/bert-sentiment/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 2.5085173, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 2.5085173, step = 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljiUspIDm4Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_input_fn = input_fn_builder(\n",
        "    features=dev_features,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiATMaKZ-ur2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.evaluate(input_fn=dev_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_input_fn = input_fn_builder(\n",
        "#    features=test_features,\n",
        "#    seq_length=max_seq_length,\n",
        "#    is_training=False,\n",
        "#    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrbTD2EJTVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  input_examples = [InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = convert_examples_to_features(input_examples, label_list, max_seq_length, tokenizer)\n",
        "  predict_input_fn = input_fn_builder(features=input_features, seq_length=max_seq_length, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "\n",
        "  #[(print(prediction)) for sentence, prediction in zip(in_sentences, predictions)]\n",
        "\n",
        "  return [(sentence, prediction['probabilities'], label_list[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-thbodgih_VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first line is from cnsnews.com\n",
        "# rest are from huffington post\n",
        "\n",
        "pred_sentences = [\n",
        "  \"“Coronavirus now, politics later – please,” Media Research Center President Brent Bozell writes in a Washington Times commentary urging the liberal media to turn their efforts from their obsession with attacking President Donald Trump to serving the greater good during a time of national crisis.\",\n",
        "  \"President Donald Trump is rejecting calls to put a single military commander in charge of medical supplies for the COVID-19 pandemic.\",\n",
        "  \"Boris Johnson is breathing without a ventilator and is in “good spirits” while being treated in intensive care for coronavirus symptoms, Downing Street has said.\",\n",
        "  \"President Donald Trump reportedly owns a stake in a company that produces hydroxychloroquine, the anti-malaria drug he has repeatedly touted as a coronavirus treatment even though his experts say there’s no strong evidence it works. \"\n",
        "  ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrZmvZySKQTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkTE8-7oQLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDhIAxvR6LG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}